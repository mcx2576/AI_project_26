{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GEOparse\n",
    "import pandas as pd\n",
    "\n",
    "def download_experiment(experiment):\n",
    "    GEOparse.get_GEO(geo=experiment, destdir=\"data/\")\n",
    "\n",
    "def process_experiment(experiment):\n",
    "    return GEOparse.get_GEO(filepath=\"data/\" + experiment + \"_family.soft.gz\")\n",
    "\n",
    "def aggregate_samples(experiment, download=True):  \n",
    "    '''\n",
    "    Wrapper function to download and process a given experiment ID,\n",
    "    aggregate the samples and write them to a csv file as a matrix.\n",
    "    '''\n",
    "    if download:\n",
    "        download_experiment(experiment)\n",
    "    print('process experiment...')\n",
    "    gse = process_experiment(experiment)\n",
    "    aggregated_samples = pd.DataFrame()\n",
    "    genes = pd.DataFrame()\n",
    "    print('aggregating samples...')\n",
    "    for sample in gse.gsms:\n",
    "        name = gse.gsms[sample].metadata['title'][0]\n",
    "        sample_df = gse.gsms[sample].table\n",
    "        sample_df.columns = ['gene', name]\n",
    "        genes = sample_df['gene']\n",
    "        aggregated_samples = pd.concat([aggregated_samples, sample_df[name]], axis=1)    \n",
    "    aggregated_samples = pd.concat([genes, aggregated_samples], axis=1)\n",
    "    aggregated_samples.to_csv('data/' + experiment + '_full_matrix.csv') \n",
    "    print('aggregated samples written to csv file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def distribution_parameters(gene, genes):\n",
    "    gene_differences = np.array([g-gene for g in genes])\n",
    "    covariance = np.cov(gene_differences, rowvar=False)\n",
    "    mean = np.mean(gene_differences, axis=0)\n",
    "    return mean, covariance\n",
    "\n",
    "def score_multivariate(x, mean, covariance):\n",
    "    return 1-multivariate_normal.pdf(x, mean=mean, cov=covariance)\n",
    "\n",
    "def score_cosine(x, y):\n",
    "    return (np.dot(x, np.transpose(y))/(np.linalg.norm(x)*np.linalg.norm(y)))\n",
    "    \n",
    "def sequential_similarities(data, similarity_function=score_cosine):\n",
    "    print(\"calculating sequential similarity...\")\n",
    "    similarities = [similarity_function(data[i], data[i+1]) for i in range(0, len(data)-1)]\n",
    "    return similarities\n",
    "\n",
    "def similarities(data, similarity_function=score_cosine):\n",
    "    print(\"calculating all similarities...\")\n",
    "    similarities = []\n",
    "    for x in data:       \n",
    "        similarities.append([similarity_function(x, y) for y in data])\n",
    "    return similarities\n",
    "\n",
    "def most_similar_samples(sample, sample_similarities, n=5):\n",
    "    print('Finding most similar samples for', sample + '...')\n",
    "    s = sample_similarities[s2i[sample]]\n",
    "    best = reversed(np.argsort(s)[-n:])\n",
    "    return [i2s[b] for b in best]\n",
    "\n",
    "def threshold_operons_sequential(similarities, threshold=0.6):\n",
    "    '''threshold a sequential list of similarities. \n",
    "    This means the spatial location of the genes is used\n",
    "    '''\n",
    "    cluster = 0\n",
    "    threshold_operons = defaultdict(list)\n",
    "    print('thresholding similarity into operons...')\n",
    "    for i, sim in enumerate(similarities):\n",
    "        if i == 0:\n",
    "            threshold_operons[cluster].append(i2g[i])\n",
    "            continue\n",
    "        if sim > threshold:\n",
    "            threshold_operons[cluster].append(i2g[i])\n",
    "        else:\n",
    "            cluster += 1\n",
    "            threshold_operons[cluster].append(i2g[i])\n",
    "    labels = [label for label, operon in threshold_operons.items() for _ in operon]\n",
    "    print('done.')\n",
    "    return labels\n",
    "\n",
    "def read_gold_operons(path):\n",
    "    operons_per_id = defaultdict(list)\n",
    "    with open(path, 'r') as infile:\n",
    "        reader = csv.reader(infile, delimiter='\\t')\n",
    "        header = next(reader)\n",
    "        for line in reader:\n",
    "            operon_id = line[0]\n",
    "            gene_id = line[2]\n",
    "            operons_per_id[operon_id].append(gene_id)\n",
    "    labels = [label for label, operon in enumerate(operons_per_id.values()) for _ in operon]\n",
    "    return labels    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data matrix...\n",
      "calculating sequential similarity...\n",
      "calculating all similarities...\n",
      "Finding most similar samples for PsigKGin1_T4.5...\n",
      "['PsigKGin1_T4.5', 'Gin1_T4.5', 'PsigFGin1_T4.5', 'GerR_3', 'SigK2']\n",
      "thresholding similarity into operons...\n",
      "done.\n",
      "The adjusted Rand score of the benchmark is: 0.0\n",
      "The adjusted Rand score of the predicted operons is: 0.19767824120251337\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "## Prepare data\n",
    "print('reading data matrix...')\n",
    "df = pd.read_csv('data/eichenberger_full_matrix.csv').fillna(0)\n",
    "gene_data = normalize(df.values[:4177, 2:].astype(float), axis=0, norm='l1')\n",
    "sample_data = normalize(df.values[:, 2:].astype(float), axis=0, norm='l1').transpose()\n",
    "\n",
    "i2s = list(df)[2:] # index to sample name\n",
    "s2i = {sample: i for i, sample in enumerate(i2s)} # sample name to index\n",
    "\n",
    "i2g = df.values[:4177, 1] # index to gene name\n",
    "g2i = {gene: i for i, gene in enumerate(i2g)} # gene name to index\n",
    "\n",
    "## Similarities\n",
    "sequential_gene_similarities_cosine = sequential_similarities(gene_data)\n",
    "sample_similarities_cosine = similarities(sample_data)\n",
    "\n",
    "## Similar samples\n",
    "random_sample = i2s[random.randint(0, len(i2s))]\n",
    "print(most_similar_samples(random_sample, sample_similarities_cosine))\n",
    "\n",
    "## Predictions\n",
    "predicted_operons = threshold_operons_sequential(sequential_gene_similarities_cosine)\n",
    "benchmark_operons = [i for i, gene in enumerate(i2g)][:-1]\n",
    "gold_operons = read_gold_operons('data/1240.opr')\n",
    "\n",
    "## Validation\n",
    "print('The adjusted Rand score of the benchmark is:',\n",
    "      adjusted_rand_score(gold_operons, benchmark_operons))\n",
    "\n",
    "print('The adjusted Rand score of the predicted operons is:',\n",
    "      adjusted_rand_score(gold_operons, predicted_operons))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
